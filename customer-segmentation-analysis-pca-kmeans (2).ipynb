{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2549419,"sourceType":"datasetVersion","datasetId":1546318}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n # Table of Contents  \n\n- Part 0:  [Project Introduction](#1)\n- Part 1:  [Exploratory Data Analysis (EDA)](#2)   \n  - 1.1  [Import files and load data](#3)\n  - 1.2  [Basic data inspectations](#4)\n  - 1.3  [Deeper EDA, Data cleaning](#5)\n    - 1.3.1 [N/As and irrelevants](#6)\n    - 1.3.2 [Datetime and new features](#7)\n    - 1.3.3 [Discrete and continuous features](#8)\n  - 1.4 [Data Visulization and analysis](#9)\n- Part 2:  [Data preprocessing](#10)\n- Part 3: [Model building and training](#11)\n  - 3.1 [PCA](#12)\n     - 3.1.1 [PCA for visualization](#13)\n     - 3.1.2 [PCA for feature engineering](#14)\n  - 3.2 [K-means Model building](#15)\n- Part 4: [Model evaluation and interpretation](#16)\n  - 4.1 [Model Evaluation](#17)\n  - 4.2 [Model Interpretation and Cluster Personality Analysis](#18)\n- Part 5: [Conclusion](#19)\n- [References](#20)\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1>  Part 0: Project Introduction</h1>\n</div> <a id=\"1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"background-color: #FCFFE9; padding: 10px;\">\n\n- **Goal** - What problem to solve?  <br>\n  - In the business context, understanding customers is crucial. This project focuses on **customer segmentation analysis using the KMeans unsupervised learning method**. The objective is to categorize customers into distinct groups based on various characteristics and behaviors, aiming to uncover unique needs and pain points within each segment. The ultimate goal is to provide **actionable recommendations** aligned with the specific preferences and characteristics identified in each customer segment\n- **ML approach**?\n  - For clustering, the unsupervised **KMeans** method will be employed. While other clustering methods could be considered, because lots of analysis happens post-modeling for this project, I will only focus on KMeans for simplicity. \n    \n  - Since KMeans is vulnerable to curse of dimensionality, we will do lots of **feature selection/engineering** including **PCA**.  \n    \n    \n- **Model tuning for unsupervised learning**?\n  - Unsupervised learning don't have labels. In this project, tuning happens in the following areas:\n    \n    - **Scaling**: although technically not tuning, scaling is key for KMeans performance. With the help of PCA visualization, a comparison of MinMax and Standard scaling will be conducted.\n    \n    - **PCA Dimensions**: will be determined using variance analysis, principal component coefficients analysis, and consideration of business interests. \n    - **Number of Clusters**: will be determined using elbow method with a practical analysis based on resutling clustering groups.\n    \n    \n\n\n  ","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"2\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1>Part 1: Exploratory Data Analysis (EDA)</h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"3\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2>Part 1.1: Import Files and Load Data</h2>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='top'></a>","metadata":{}},{"cell_type":"code","source":"# Import Files\n# General\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport os\n\n# sklearn\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# scipy\nfrom scipy.stats import pointbiserialr\n\n# others\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\n\n# Check directories and file names\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# load files \ndata = pd.read_csv('/kaggle/input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"4\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2>Part 1.2: Basic data inspectations</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Basic data inspections\npd.set_option('display.max_columns', None)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use info() to get some basic informations (data size, column types)\ndata.info()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at some statistics. Since there are many columns, I break down the output by 2 parts.","metadata":{}},{"cell_type":"code","source":"# Some statistics (part1)\ndata.loc[:,'Education':'MntGoldProds'].describe(include='all').style.background_gradient(cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some statistics (part2)\ndata.loc[:,'NumDealsPurchases':].describe(include='all').style.background_gradient(cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check dups, or NAs\nprint('duplicates: ',data.duplicated().sum())\nprint('NAs: \\n', data.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:100%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\n    \n- Dataset has **2240 rows** and **29 columns**.\n\n- As describedüëá, dataset has attributes in 4 areas: people, products, promotions, and places.\n\n- Columns like **'Z_CostContact'** and **'Z_Revenue'** have same values and provide no useful information. They can be safely deleted.\n\n- **'Income'** has 24 **null values**.\n\n- **'Dt_Customer'** is in object format and needs to be converted. ","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"background-color: #FCFFE9; padding: 10px;\">\n    \n## Attributes (Check details [here](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis))\n\n### Peopleüë®‚Äçüë©‚Äçüëß\n\n- **ID:** Customer's unique identifier\n- **Year_Birth:** Customer's birth year\n- **Education:** Customer's education level\n- **Marital_Status:** Customer's marital status\n- **Income:** Customer's yearly household income\n- **Kidhome:** Number of children in the customer's household\n- **Teenhome:** Number of teenagers in the customer's household\n- **Dt_Customer:** Date of customer's enrollment with the company\n- **Recency:** Number of days since the customer's last purchase\n- **Complain:** 1 if the customer complained in the last 2 years, 0 otherwise\n\n### Productsüç∑\n\n- **MntWines:** Amount spent on wine in the last 2 years\n- **MntFruits:** Amount spent on fruits in the last 2 years\n- **MntMeatProducts:** Amount spent on meat in the last 2 years\n- **MntFishProducts:** Amount spent on fish in the last 2 years\n- **MntSweetProducts:** Amount spent on sweets in the last 2 years\n- **MntGoldProds:** Amount spent on gold in the last 2 years\n\n### Promotionüí¥\n\n- **NumDealsPurchases:** Number of purchases made with a discount\n- **AcceptedCmp1:** 1 if the customer accepted the offer in the 1st campaign, 0 otherwise\n- **AcceptedCmp2:** 1 if the customer accepted the offer in the 2nd campaign, 0 otherwise\n- **AcceptedCmp3:** 1 if the customer accepted the offer in the 3rd campaign, 0 otherwise\n- **AcceptedCmp4:** 1 if the customer accepted the offer in the 4th campaign, 0 otherwise\n- **AcceptedCmp5:** 1 if the customer accepted the offer in the 5th campaign, 0 otherwise\n- **Response:** 1 if the customer accepted the offer in the last campaign, 0 otherwise\n\n### Placeüöå\n\n- **NumWebPurchases:** Number of purchases made through the company‚Äôs website\n- **NumCatalogPurchases:** Number of purchases made using a catalogue\n- **NumStorePurchases:** Number of purchases made directly in stores\n- **NumWebVisitsMonth:** NumWebVisitsMonth: Number of visits to company‚Äôs website in the last month","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"5\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 1.3: Deeper EDA, data cleaning, and new features</h2>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #CAEFD1; padding: 10px;\">\n<span style=\"font-size: larger;\">\n\n- Part 1.3.1: Basic data cleaning  <br>\n    (Additional cleaning and feature creation happen during data preprocessing for modeling purposes.)\n    \n  - Remove **irrelevant columns**  \n    \n  - Inspect and clean **missing values** \n     \n- Part 1.3.2: **Datetime and new features** <br>\n    \n  - Datetime features usually need some transformation so we can extract some useful information from them. We'll look at field such as **'Dt_Customer'**.<br>\n    <br>\n    \n- Part 1.3.3: Seperate **discrete and continuous** features \n    \n  - Numerical feature without many unique values are discrete features. It's easier to visualize and analyze them as categorical features.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"6\"></a>\n<div style=\"text-align: center; background-color: #FDE0D9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 1.3.1: Get rid of N/As and irrelevants</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"#### Drop NAs","metadata":{}},{"cell_type":"code","source":"# Drop columns \"Z_CostContact\" and \"Z_Revenue\". They aren't meaningful since all entries are the same.\ndata.drop([\"Z_CostContact\" , \"Z_Revenue\"] ,axis=1, inplace=True)\n\n# Drop \"ID\" also, they aren't useful. \ndata.drop([\"ID\"] ,axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We've known there are 24 records with null values.Take a look.\ndata[data['Income'].isnull()].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seems there are nothing special about them. Since there aren't many, exclude them.\ndata.dropna(subset=['Income'],axis=0, inplace=True)\n\n# Check number of records left\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"7\"></a>\n<div style=\"text-align: center; background-color: #FDE0D9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 1.3.2: Datetime and new features</h2>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"#### 'DT_Customer' feature","metadata":{}},{"cell_type":"code","source":"# Convert 'Dt_customer' to datetime type\ndata['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'], format=\"%d-%m-%Y\")\n# Check data type after converting\nprint('Dt_Customer type: ',data[\"Dt_Customer\"].dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To create seniority, we need to find a reasonable point of time for calculation so we dive in  the 'Dt_Customer' field a bit\n\n# Check the earliest and latest date time\nprint('Earliest customer enrollment date: ', data['Dt_Customer'].min())\nprint('latiest customer enrollment date: ', data['Dt_Customer'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create 'Seniority' feature\nOkay, Dt_Custeomer' ranges from around mid of 2012 to 2014. \nI think using latest customer enrollment date as the date point for calculation is reasonable. I would like to use it in weeks.","metadata":{}},{"cell_type":"code","source":"# Create 'Seniority'(in weeks) feature. \nlast_date =  data['Dt_Customer'].max()\ndata['Seniority'] = (last_date -data['Dt_Customer']).dt.days/7\n\n# Take a look to make sure calculation is correct\ndata[['Dt_Customer','Seniority' ]].head(2)\n\n# 'Dt_Customer' is now redundant. Remove\ndata.drop(['Dt_Customer'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create 'Age' feature.\n# I am not sure when the data is collected but it could be in 2021. Let's just use 2021 as 'today' to calculate the age.\ntoday = 2021\ndata['Age'] = today - data['Year_Birth']\n\n# 'Year_Birth' is now redundant. Remove\ndata.drop(['Year_Birth'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"8\"></a>\n<div style=\"text-align: center; background-color: #FDE0D9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 1.3.3: Discrete and continuous features</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Divide features by discrete and continuous\n\n# Function to seperate discrete and continuous features in original data (won't work after PCA or Clustering)\ndef divide_feature_types(data):\n        '''\n        inpute a data frame and output continuous, and discrete feature columns in list format\n        '''\n        # Initialize\n        col_cont=[] \n        col_dis=[] \n        # loop through and seperate columns\n        for c in data.columns: \n            if ('home' in c) or ('Num' in c) or ('Accepted' in c) or (c=='Complain') or (c=='Response'): \n                col_dis.append(c)\n            elif (data[c].dtype=='O'):\n                col_dis.append(c)\n            else:\n                col_cont.append(c)\n        return col_cont, col_dis\n\n# Call the function and get column lists\ncol_cont, col_dis = divide_feature_types(data)\nprint('Continuous numerical features: ', col_cont)\nprint('Categorical or discrete features: ', col_dis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"9\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 1.4: Data visualization and analysis</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FCFFE9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Continuous Features </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Histogram and kdeplot to check overall distribution first.","metadata":{}},{"cell_type":"code","source":"# Histogram and kdeplot for all continuous features \ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_cont) // grh_per_row , grh_per_row, figsize=(30, 60))\n\nfor count, feature in enumerate(col_cont, 0):\n    data_copy = data.copy()\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n\n    # Plot histogram on the primary y-axis\n    ax[row, col].hist(data_copy[feature], bins=50, alpha=0.7, color='skyblue', label='Histogram')\n    ax[row, col].set_xlabel(feature, fontsize=30)\n#     ax[row, col].set_ylabel(\"Histogram Count\", color='blue', fontsize=30)\n#     ax[row, col].legend(loc='upper left', fontsize=30)\n    \n    # Plot KDE plot on the secondary y-axis\n    ax2 = ax[row, col].twinx()\n    sns.kdeplot(data_copy[feature], fill=True, color='pink', ax=ax2, label='KDE Plot')\n#     ax2.set_ylabel(\"KDE Density\", color='red', fontsize=30)\n#     ax2.legend(loc='upper right', fontsize=30)\n    ax[row, col].tick_params(axis='both', labelsize=16)\n    ax2.tick_params(axis='both', labelsize=16)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seems income and age follow sort of some normal distributions. That's good. Overall, customers seem to be mostly mid age to senior. Seniority and recency seems to be quite uniform with some high and lows but nothing too strange. For amounts purchased (fields start with 'Mnt'), the distribution lean and concentrate at the very low (near 0) end. This could be something we need to dig. \n### Next, I want to see  how the distribution of continuous features vary by some discrete features. Let's take a look at some boxplot by education, marital status, kidhome, teenhome.","metadata":{}},{"cell_type":"code","source":"# Boxplot for all continuous features - by education\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_cont)//grh_per_row, grh_per_row, figsize=(35, 60))\n\nfor count, feature in enumerate(col_cont, 0):\n    data_copy = data.copy()\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n\n    sns.boxplot(x='Education', y=feature, data=data_copy, ax=ax[row, col])\n\n    # Set labels and axis labels font size\n    ax[row, col].set_xlabel('Education', fontsize=30)\n    ax[row, col].set_ylabel(feature, fontsize=30)\n    ax[row, col].tick_params(axis='both', labelsize=30)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Boxplot for all continuous features - by marital status\ngrh_per_row =2\nfig, ax = plt.subplots(len(col_cont)//grh_per_row,grh_per_row, figsize = (35, 80))\n\nfor count, feature in enumerate(col_cont, 0):\n       data_copy= data.copy()\n       row =count // grh_per_row\n       col=(count )% grh_per_row    \n       sns.boxplot(x='Marital_Status', y=feature, data=data_copy, ax=ax[row, col])\n        \n       # Set labels and axis labels font size\n       ax[row, col].set_xlabel('Marital Status', fontsize=30)\n       ax[row, col].set_ylabel(feature, fontsize=30)\n       ax[row, col].tick_params(axis='both', labelsize=30, rotation=30)\n\nplt.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Boxplot for all continuous features - by Kidhome\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_cont)//grh_per_row,grh_per_row, figsize = (35, 60))\n\nfor count, feature in enumerate(col_cont, 0):\n       data_copy= data.copy()\n       row =count // grh_per_row\n       col=(count )% grh_per_row    \n       sns.boxplot(x='Kidhome', y=feature, data=data_copy, ax=ax[row, col])\n       ax[row, col].tick_params(axis='x', rotation=30)  # Rotate x-axis labels\n        \n       # Set labels and axis labels font size\n       ax[row, col].set_xlabel('Kid home', fontsize=30)\n       ax[row, col].set_ylabel(feature, fontsize=30)\n       ax[row, col].tick_params(axis='both', labelsize=30)\nplt.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Boxplot for all continuous numerical features - by Teen home\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_cont)//grh_per_row,grh_per_row, figsize = (35, 60))\n\nfor count, feature in enumerate(col_cont, 0):\n       data_copy= data.copy()\n       row =count // grh_per_row\n       col=(count )% grh_per_row    \n       sns.boxplot(x='Teenhome', y=feature, data=data_copy, ax=ax[row, col])\n        \n       # Set labels and axis labels font size\n       ax[row, col].set_xlabel('Teenhome', fontsize=30)\n       ax[row, col].set_ylabel(feature, fontsize=30)\n       ax[row, col].tick_params(axis='both', labelsize=30)\n\nplt.show()","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FCFFE9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Discrete Features </h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Countplot for discrete features\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_dis) // grh_per_row, grh_per_row, figsize=(35, 120))\n\nfor count, feature in enumerate(col_dis, 0):\n    data_copy = data.copy()\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n    sns.countplot(x=feature, data=data_copy,  ax=ax[row, col])  # Specify the axis for the countplot\n    ax[row, col].set_xlabel(feature)\n    ax[row, col].set_ylabel(\"Count\") \n    \n   # Set labels and axis labels font size\n    ax[row, col].set_xlabel(feature, fontsize=30)\n    ax[row, col].set_ylabel(feature, fontsize=30)\n    ax[row, col].tick_params(axis='both', labelsize=30, rotation=30)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:80%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\n    \n#### Distribution plots and statistic table:\n  - Most **product sales** in the past 2 years are **heavily concentrated towards the lower end**. This trend is consistent with our initial data inspection. Specifically, the 25th percentile values for wine, meat, gold, fish, fruit,sweets, and are 23, 16, 9,3,1, 1. This suggests that a significant portion of customers made minimal purchases especially toward products other than wine and meat, indicating a **potential lack of repeat visits** or the store **doesn't sell much stuff except wines and meats**. Also, since this project we will use customer segmentation, it's good to keep this in mind and discover later which group contribute this abnormal pattern the most.\n    \n  - **Wine sales the best** (with mean of 304 and median of 174). The next one is **meat** (mean of 38 and median of 67).\n    \n  - Overall, the **customers are be mid aged to senior** (interquartile of 45 to 65).\n    \n    \n#### Boxplots:\n  - Produce **sales don't differ with education and marital status** too much. In wine sales, phd might spend more but it could relate to their a bit higer income.\n    \n  - People with **young kid (not teen) seem to spend less on wines**. \n    \n  - There are a few **outliers**. (I didn't include outliers with exceptional large meat or sweet purchases because I suspect this is a wine store so those side/small product sales aren't my focus. I may combine or even drop those columns later):\n  - 'Income'>600000 \n  - 'Age'>100 \n    \n#### Countplots:\n  - Around **65%** of customers **have a partner** (married or together)\n    \n  - Around **40%** of customers **have kids**.\n    \n  - A little less than **50%** customers **have teens**.\n    \n  - **Most people (>95%) at least have one deal purchase** but it concentrates at **1 deal purchase (around 40%)**.\n    \n  - Overall, people do slightly **more store purchase than than the web. The least one is catalog**.\n    \n  - Number of **web visits last month** concentrated at the **range of 5-8**.\n    \n  - Campaign acception rate overall is low.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"10\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1> Part 2: Data preprocessing for modeling</h1>\n</div> <a id=\"1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #CAEFD1; padding: 10px;\">\n<span style=\"font-size: larger;\">\n\n### K-Means, Curse of dimensionality, and feature selection:   üí°\n  - Depends on how the learning model can handle the consequence, **Curse of dimensionality** can be quite harmful because:\n    - Data becomes **sparse**\n    - Features tends to be **redundant and correlated to overfit**\n  - **Clustering** methods such as **K-means heavily rely on distance measures**. They are very **vulnerable to Curse of Dimensionality** because the higher the dimension, the less meaningful and reliable the distance measure. When some meaningless features 'participate' in the distance measure, the clustering assignment would become difficult! So, we should **aim to keep the # of features low** and retain only the meaningful ones.  \n  - **Preprocessing using PCA** may help.\n\n### Categorical feature encoding:  üí°\n  - We need to do label encoding on categorical features because machine learning model can only accept numerical values. .\n  - Two main types of categorical variables:\n    - **Ordinal**: has a predefined order (Ex: small, medium, and large). Encoding methods like **label encoding and ordinal encoder** are suitable.\n    - **Nominal**: has no predefined order (Ex: man and woman). Encoding methods like **one-hot encoding and binary encoding** are appropriate. Note one-hot encoding can lead to a high number of fields, potentially causing **overfitting**.\n\n### Outliers: üí°\n   Clustering methods like K-means are **sensitive to outliers** because it minimizes the sum of squared distances from each point to the centroid. Outliers can disproportionately **influence the centroid positions**.\n    \n### Feature scaling:   üí°\n   - Clustering is distance based model. If the scaling is very different (say income vs. sale), the measuring would focus on large scaled features.\n   - I will try 2 popular scaling:\n     - **MinMax Scaling**: scales the data linearly to a specific range, typically between 0 and 1. The formula is (x‚àímin(X))/(max(X)‚àímin(X)). It's effective when the data distribution is not necessarily normal but it's **sensitive to outliers** as the entire range is influenced by the minimum and maximum values.\n     - **Standard Scaling**: the scaled data have a **mean of 0 and standard deviation of 1**. It is suitable for algorithms that assume a normal distribution of the input features and **is less sensitive to outliers**. \n     - Since each has it's pros and cons, I will try both and do a **2-D PCA to visualize** and decide which one to use.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #CAEFD1; padding: 10px;\">\n<span style=\"font-size: larger;\">\n\n\nWith the aim to **keep the number of features low and meaningful**, let's plan data preprocessing for this project:\n    \n### Feature Selection:  üìÖ <br>\n  - '**AcceptedCamp' 1,2,3,4,5 and 'Response'** are quite sparse and convey similar information. Which campaign the customer accept isn't my focus. I'll create a **'Campaigns_Accepted'** field to **count the total** and exclude the rest.\n    \n  - **Fish, Fruit, Sweets, and Gold** are sold in very **small amounts** (suspecting this is a wine store with these as side products). Leaving them in would only confuse the distance measurement for customer clustering. I will **exclude them**. \n    \n### Categorical Feature Encoding:   üìÖ <br>\n   - **'Marital_Status'** has too many options. I am not interested in seeing very small subgroup behaviors for this project. I'll need to use one-hot encoding if I keep them all (because it's nominal), which would add too many unecessary dimensions. I'll divide it into **'coupled' vs. 'not coupled'**.\n    \n   - **'Education'** is an ordinal variable, so I will use a **label encoder**.\n    \n### Outliers:üìÖ <br>\n   - Remove the ones identified above.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### **Feature selection and transformation**","metadata":{}},{"cell_type":"code","source":"# Combine Campaingns_Accepted\ndata[\"Campaigns_Accepted\"] = data['AcceptedCmp3']+ data['AcceptedCmp4']+ data['AcceptedCmp5']+data['AcceptedCmp1']+data['AcceptedCmp2'] +data['Response']\ndata.drop(['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2',  'Response'], axis=1, inplace=True)\n\n# Drop purchases other than wines and meats\ndata.drop(['MntFruits','MntFishProducts', 'MntSweetProducts', 'MntGoldProds'], axis=1, inplace=True)\n\n# 'Maritcal_Status' to be bianary\ndata['Marital_Status']= data['Marital_Status'].replace(['Alone', 'Absurd', 'YOLO', 'Widow','Single','Divorced'],'Not Coupled')\ndata['Marital_Status']= data['Marital_Status'].replace(['Married', 'Together'],'Coupled')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Remove Outliers**","metadata":{}},{"cell_type":"code","source":"# Remove outliers\nmask = ( data['Income'] <= 600000) & (data['Age'] <= 100) \ndata = data[mask]\n\n# Check record numbers after removel\nprint('Data shape after removing outliers: ', data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Catogorical label encoding**","metadata":{}},{"cell_type":"code","source":"# Encode categorical features\ncategorical_features = ['Education', 'Marital_Status']\nle=LabelEncoder()\nfor i in categorical_features:\n    data[i]=data[[i]].apply(le.fit_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Feature Scaling**\nI will try both **\"MinMax\" and \"Standard\"** and see how they do. ","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\n# MinMax\nscaler = MinMaxScaler()\nscaler.fit(data)\ndata_MinMax = pd.DataFrame(scaler.transform(data),columns= data.columns )\n\n# Standard\nscaler = StandardScaler()\nscaler.fit(data)\ndata_std = pd.DataFrame(scaler.transform(data),columns= data.columns )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Heatmap** to check correlations","metadata":{}},{"cell_type":"code","source":"# Now all features are numerical, I can use heatmap to check feature correlations\ncorr = data.corr(method='spearman')\nplt.figure(figsize=(22, 20))\nplt.title(\"Customer data feature correlation\", fontsize=24)\nsns.heatmap(data=corr, annot=True, cmap=\"BuPu\", annot_kws={\"size\": 16})\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:80%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\n    \n- A few **moderate to highly correlated features** (I define |r| > 0.7 as Strong, 0.5<|r|<0.7: Moderately):\n  - **'Income'Ôºå 'MntWines','MntMeatProducts', 'NumCatalogPurchases', 'NumStorePurchases'** have **strong positive correlations**.\n    \n  - **'Kidhome'** has **moderate negative correlations** with **'Income'** and **all products purchases**.\n    \n  - **'Kidhome'** has **moderate positive correlations** with **'NumWebVisitsMonth'**. (People with kids visit the webs a lot but hesitate to make purchases? Who are they?)\n    \n  - **'Teenhome'** has **moderate positive correlations** with **'NumDealsPurchases'** (People with teens do more deal purchases? What cause this? Note correlation isn't the same as causation)\n    \n  - **'NumWebVisitsMonth'** has **moderate negative correlations** with **'NumCatalogPurchases', 'NumStorePurchases'** (People visit web a lot tends to do less catalog or store purchase? Who are they?)\n   \n  - **'NumWebVisitsMonth'** has **moderate positive correlations** with **'Kidhome'** (as mentioned above) and **'NumDealsPurchases'**. (People with kids like to visit webs? People visit webs more do more deal purchases?)\n ","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"11\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1> Part 3: Model building and training</h1>\n</div> <a id=\"1\"></a>","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"12\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 3.1: Principal Component Analysis (PCA)</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #CAF4F4; padding: 10px;\">\n<span style=\"font-size: larger;\">\n\n### What is PCA?\n- PCA is a **dimensionality reduction technique**. It transforms the original features of a dataset into a new set of **uncorrelated features** called principal components.\n\n- These principal components are **linear combinations of the original features**.\n- The best vector (measured either by preserving the maximum variance or the mean squared distance between the original and projected data) to project onto is called the **1st principal component**. \n    \n- Subsequent principal components capture the **largest remaining variance** and are **orthogonal** to those that precede them.\n    \n- This orthogonality ensures that each principal component captures a unique aspect of the data's variability.\n    \n    \n### How PCA works mathematically?\n- We can do **eigen decomposition** if a matrix is **squared** (that is, a n * n same width and height matrix).\n    \n- The formula is **A=PŒõP^‚àí1**, where **P** is the matrix whose columns are the **eigenvectors** of A and **Œõ** is a **diagonal matrix** whose diagonal elements are the **eigenvalues** of A. P^-1 is the inverse of P. \n    \n- When matrix A is squared and **symmetric** (which is the case when we calculate the features' correlation matrix), P^-1 and P are **orthogonal** to each other. This means that **each principal component is uncorrelated with every other principal component**, which is the key characteristic of PCA and contributes to its effectiveness in capturing the maximum variance in the data.\n    \n    \n### How to choose the number of principal components?\n- The eigen values in **diagonal matrix Œõ** represents the **ordered variance** (from large to small) preserved by each eigen vectors. So, we can choose the **number of principal components based %variance we want to preserve**. For example, with k dimentions, the preserved variance is Œª1+Œª2+‚Ä¶+Œªk (by adding up the first k values in the diagonal matrix  Œõ) / The total variance (which is the sum of all element in  Œõ.)\n- However, sometime variance isn't everything. The effectiveness of a principal component for clustering also depends on whether it help clustering by meaningful features. **PCA aims to maximize variance, but it doesn't necessarily consider the specific information relevant for clustering**. \n    \n### What's the use of PCA?\n- Reduce data to a much **lower dimension (2 or 3) to help visualization**\n- Reduce dimension before modeling to **help curse of dimensionality** \n\n    \n\n    \n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"13\"></a>\n<div style=\"text-align: center; background-color: #FDE0D9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 3.1.1: PCA for visualization</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Let's **build and visualize PCA** for MinMax and Standard scaled data. We will then look at PCA in 2-D and decide which scaling method to use. ","metadata":{}},{"cell_type":"code","source":"# PCA with 2-D for visualization\n\n# MinMax scaled data\npca_MinMax = PCA(n_components=2)\npca_MinMax.fit(data_MinMax)\nPCA_viz_MinMax = pd.DataFrame(pca_MinMax.transform(data_MinMax), columns=([\"Dimension 1\",\"Dimension 2\"]))\n\n# Standard scaled data\npca_std = PCA(n_components=2)\npca_std.fit(data_std)\nPCA_viz_std =    pd.DataFrame(pca_std.transform(data_std), columns=([\"Dimension 1\",\"Dimension 2\"]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize data with 2-D PCA with scaled data\nfig, axes = plt.subplots(2, 2, figsize=(20, 20))\n\n\n# MinMax scaling\n# Scatter plot \nsns.scatterplot(x=PCA_viz_MinMax[\"Dimension 1\"], y=PCA_viz_MinMax[\"Dimension 2\"], ax=axes[0][0])\naxes[0][0].set_title('PCA: Scatter Plot - MinMax Scaled')\naxes[0][0].set_xlabel('Dimension 1')\naxes[0][0].set_ylabel('Dimension 2')\n\n# KDE plot \nsns.kdeplot(x=PCA_viz_MinMax [\"Dimension 1\"], y=PCA_viz_MinMax[\"Dimension 2\"], cmap='viridis', fill=True, ax=axes[0][1])\naxes[0][1].set_title('PCA: Kernel Density Plot- MinMax Scaled')\naxes[0][1].set_xlabel('Dimension 1')\naxes[0][1].set_ylabel('Dimension 2')\n\n\n#Standard scaling\n# Scatter plot \nsns.scatterplot(x=PCA_viz_std[\"Dimension 1\"], y=PCA_viz_std[\"Dimension 2\"], ax=axes[1][0])\naxes[1][0].set_title('PCA: Scatter Plot - Standard Scaled')\naxes[1][0].set_xlabel('Dimension 1')\naxes[1][0].set_ylabel('Dimension 2')\n\n# KDE plot \nsns.kdeplot(x=PCA_viz_std [\"Dimension 1\"], y=PCA_viz_std[\"Dimension 2\"], cmap='viridis', fill=True, ax=axes[1][1])\naxes[1][1].set_title('PCA: Kernel Density Plot- Standard Scaled')\naxes[1][1].set_xlabel('Dimension 1')\naxes[1][1].set_ylabel('Dimension 2')\n\n# Adjust ticks font size\nfor ax in axes.flatten():\n    ax.tick_params(axis='both', labelsize=14)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:80%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\n\n## MinMax or Standard?\n- Observation: <br>\nIt seems **MinMax scaling is unsuitable** for this dataset, leading to two distinct clusters with a substantial distance of around 1. This discrepancy is attributed to the way MinMax scaling treats binary features, particularly by maintaining a constant distance of 1 between individuals in different categories.\n\n- Explanation: <br>\nGiven the dataset's **predominantly continuous** nature with a **few discrete** variables, MinMax scaling tends to **overly emphasize the influence of these discrete features**(especially 0/1 binary). The **minimal variability in binary features** can result in an **imbalanced feature importance**.\n\n- Decision:\nI will use **Standard Scaling** because it provide a more balanced representation of feature importance.\n    \n## Three clusters?\n- Seems 3 clusters may work? Keep in mind here we have 2-D PCA. This may not be true with a different # of dimensions.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"14\"></a>\n<div style=\"text-align: center; background-color: #FDE0D9; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 3.1.2: PCA for feature engineering</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"We've chosen the right scaling method. Now, let's discover **how many dimensions we want to keep for PCA**.","metadata":{}},{"cell_type":"code","source":"# Let's see %of variance vs. #of principal components we choose\ncum_vars=[]\ncomponent_coefs=[]\nfor i in range(1, len(data_std.columns)+1): \n    pca = PCA(n_components=i)\n    pca.fit(data_std)\n\n    # Get the cumulative explained variance\n    component_coefs.append(pca.components_[i-1,:])\n    cum_var = np.cumsum(pca.explained_variance_ratio_)[-1]\n    cum_vars.append(cum_var)\n    \n# Plot cumulative explained variance vs. number of components\nplt.plot(range(1, len(data_std.columns)+1), cum_vars, marker='o', linestyle='-', color='b')\n\n# Add labels and title\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance vs. Number of Principal Components')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, seems the variance wouldn't be explained by a few dimensions. Thus, we **can't simply use the variance to choose # of dimensions**. \nIn this case, a closer look at **how PCA built each principal components** (hence, the coefficients) would help us the decide. We want the model to include the dimensions that we think are relevant for clustering.","metadata":{}},{"cell_type":"code","source":"# Extract feature names\nfeature_names = data_std.columns\n\n# Initialize an empty DataFrame with a multi-index\ncomponents_df = pd.DataFrame(index=feature_names)\n\n# Iterate through each principal component\nfor i, component_coefs in enumerate(pca.components_):\n    # Create a DataFrame for the current component\n    component_df = pd.DataFrame({f'Coef_Component_{i+1}': component_coefs}, index=feature_names)\n    \n    # Concatenate the DataFrame to the main DataFrame\n    components_df = pd.concat([components_df, component_df], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at principal components coefficients\ncomponents_df.style.background_gradient(cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:80%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\nIn Principal Component Analysis (PCA), the coefficients of the eigenvectors in the pca.components_ attribute represent the loading of each original feature on the corresponding principal component. These coefficients indicate the contribution of each feature to the principal component and can be interpreted as the weights or importance of the features in constructing that principal component.\n    \n- 1st component: largest positive coefficients are on **income, MntWines, MntMeatProducts, NumCatalogPurchases, NumStorePurchases**. As we've seen in heatmap, those are highly correlated features. If the correlations are perfect (all 1s or move together in a linear fashion), we could know all the other variables when we know just one variable. In this case, we could chose 1 combined dimension that catches the largest variance and project all points to that dimension. Here, that dimension is the 1st component. So,the **higher value on this dimension would mean people with higher income, buy more wines and meat products, do more of catelog and store purchases**.\n\n- 2nd component: largest positive coefficients are on **NumDealsPurchases,Teenhome, NumWebPurchases, NumWebVisitsMonth**. The higher value on this dimension would mean people who are more active on webs, have more teen at home, do more deal and web purchases, and visit web more often.\n\n- 3rd component: largest positive coefficients are on **Seniority, NumWebVisitsMonth, Campaigns_Accepted**.\n\n- 4th, 5th, 6th, 7th,8th, and 9th component put heavy positive coefficients on **recency, marital status, complain, education, kidhome, and campaign accepted**. As we've seen in the heatmap, most of those feature aren't highly correlated to other features so those features won't be captured in the previous PCA components. Those are all interesting dimensions that would help us discover interesing cluster personalities. I wil include them. \n\n- The first 9 components capture a little less than **85% variance**. I think it's good. \n    \n- Components after 9 seem to be redundant or uninteresting. I will leave them out.","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"15\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 3.1: KMeans model building</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## How many clusters do we want for K-means? üí™Elbow method might help.\n#### Let's build PCA with 9 dimensions and check 'elbow' picture to decide.","metadata":{}},{"cell_type":"code","source":"# Elbow method to decide on clustering #s\n\n# PCA with 9 dimension\npca_std = PCA(n_components=9)\npca_std.fit(data_std)\nPCA_std =  pd.DataFrame(pca_std.transform(data_std)) \n\n# Get within-cluster-sum-of-squares for Elbow Method \nwcss = []  \n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(PCA_std)\n    wcss.append(kmeans.inertia_)\n\n# Plot the Elbow Method graph\nplt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS (Within-Cluster-Sum-of-Squares)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seems it's 3 or 4 clusters. I've tried and clusters of 4 tends to give me a very small group of only around 20 people. Not good! I will use **3 clusters**.","metadata":{}},{"cell_type":"code","source":"# Run K-means with 3 clusters (and PCA with 9 dimensions)\n\n# PCA \npca_std = PCA(n_components=9)\npca_std.fit(data_std)\nPCA_std =  pd.DataFrame(pca_std.transform(data_std)) \n\n# Kmeans and predictions\nkmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\nyhat_km = kmeans.fit_predict(PCA_std)\ndata[\"Clusters\"]= yhat_km","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"16\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1> Part 4: Model Evaluations and Interpretations</h1>\n</div> <a id=\"1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"17\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 4.1: Model Evaluation</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### First, let's take a look at clustering groups in 2-D PCA. ","metadata":{}},{"cell_type":"code","source":"# Plot clustering groups in 2-D PCA\n\n# 2-D PCA\npca_std = PCA(n_components=2)\npca_std.fit(data_std)\nPCA_viz_std =    pd.DataFrame(pca_std.transform(data_std), columns=([\"Dimension 1\",\"Dimension 2\"]))\nx, y = PCA_viz_std['Dimension 1'],  PCA_viz_std['Dimension 2']\n\n# Define custom colors and colormap for each cluster\ncluster_colors = ['darkred', 'g', 'darkblue']\ncustom_cmap = ListedColormap(cluster_colors)\n\n# Plotting the clusters in 2D\nscatter = plt.scatter(x, y, s=40, c=data[\"Clusters\"], marker='o', cmap=custom_cmap)\nplt.title(\"KMeans Cluster Groups in 2-D PCA\")\nplt.xlabel('Dimension 1 -  income, Mnt Wines/Meat, Num Catalog/Store Purchases')\nplt.ylabel('Dimension 2 - Deals/Web Purchases, Teenhome, WebVisits')\n\n# Adding colorbar with discrete color labels\ncbar = plt.colorbar(scatter, ticks=np.arange(len(cluster_colors)), boundaries=np.arange(len(cluster_colors) + 1) - 0.5, orientation='vertical')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I like what I see. The group counts are pretty even. There seem to be 3 distinct groups:\n\nLet's check how many people are in each clusters next. üëá","metadata":{}},{"cell_type":"code","source":"# Create the countplot\ncustom_palette = {0: 'darkred', 1:'g', 2: 'darkblue'}\n\nplt.figure(figsize=(5, 6))\npl = sns.countplot(x=data[\"Clusters\"], palette=custom_palette)\n\n# Annotate each bar with its count\nfor p in pl.patches:\n    pl.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)\nplt.grid(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"18\"></a>\n\n<div style=\"text-align: center; background-color: #81badd; font-size:70%; padding: 2px;border-radius:10px;\">\n    <h2> Part 4.2: Model Interpretation and cluster personality analysis</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #CAF4F4; padding: 10px;\">\n<span style=\"font-size: larger;\">\n    \n- We've seen that the cluster seems to divide customers to group that seem to look good in 2-D and the portions of each groups are just right. :) \n- Time to see if we can get find some **interesting characteristicsüé≠ of each group** and **what actions the store might take** accordingly. This is the goal of this project and is one of the **key tests** to see if PCA and Kmeans models work well.\n- I will do some visualizations and conclude what I saw at the end.","metadata":{}},{"cell_type":"code","source":"# Divide features again by continuous vs. discrete\ncol_cont = ['Income', 'Recency', 'MntWines', 'MntMeatProducts', 'Seniority', 'Age']\ncol_dis = ['Education', 'Marital_Status', 'Kidhome', 'Teenhome', 'NumDealsPurchases', 'NumWebPurchases',\n           'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Complain', 'Campaigns_Accepted', 'Clusters']\n\n# Histogram and KDE plot for continuous features\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_cont) // grh_per_row, grh_per_row, figsize=(35, 35))\n\nfor count, feature in enumerate(col_cont, 0):\n    data_copy = data.copy()\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n\n    # Plot histogram with KDE on the primary y-axis\n    sns.histplot(data_copy, x=feature, bins=50, kde=True, color='skyblue', ax=ax[row, col])\n    ax[row, col].grid(False)\n    ax[row, col].set_ylabel(\"\")\n\n    # Plot KDE on the secondary y-axis\n    ax2 = ax[row, col].twinx()\n    sns.kdeplot(data_copy[data['Clusters'] == 0][feature], color='red', ax=ax2, label='Cluster 0')\n    sns.kdeplot(data_copy[data['Clusters'] == 1][feature], color='green', ax=ax2, label='Cluster 1')\n    sns.kdeplot(data_copy[data['Clusters'] == 2][feature], color='blue', ax=ax2, label='Cluster 2')\n    ax[row, col].set_xlabel(feature, fontsize=30)\n    ax2.legend(loc='upper right', fontsize=30)\n    ax2.grid(False)\n    ax2.set_ylabel(\"\")\n\n    # Adjust axis label size and tick label size\n    ax[row, col].tick_params(axis='both', labelsize=30)\n    ax2.tick_params(axis='both', labelsize=30)\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Violin plot for continuous features\n# Number of subplots per row\ngrh_per_row = 2\n\n# Create subplots\nfig, ax = plt.subplots(len(col_cont) // grh_per_row, grh_per_row, figsize=(35, 50))\n\n# Loop through each continuous numerical feature\nfor count, feature in enumerate(col_cont, 0):\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n    ylim = max(data[feature].max(), abs(data[feature].min()))\n\n    # Plot violin plot with specific colors for each cluster\n    sns.violinplot(x='Clusters', y=feature, data=data, ax=ax[row, col], palette={0: 'darkred', 1: 'g', 2: 'darkblue'})\n    ax[row, col].tick_params(axis='x', rotation=30)  # Rotate x-axis labels\n    ax[row, col].set_title(f'Violin Plot: {feature}')\n    ax[row, col].set_title('')\n\n    # Adjust axis label size and tick label size\n    ax[row, col].tick_params(axis='both', labelsize=30)\n    ax[row, col].set_xlabel('Clusters', fontsize=30)\n    ax[row, col].set_ylabel(feature, fontsize=30)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A few quick findings - Histogram, KDE, violin:\n- Cluster 0 has the lowest income and cluster 2 has the highest.\n- Cluster 0 has lots of low end (near 0) purchases of wine and meat. Cluster 1 is the middle and cluster 2 makes the highest purchases.\n- Cluster 1 has more of longtime (80-100 weeks) customers. Cluster 0 has slightely more new (10-20 weeks) customers.\n- Cluster 0 has slightly more younger people (mid 40 to 50) and cluster 1 has slightly more older people (60-70).","metadata":{}},{"cell_type":"markdown","source":"### See some statistics to to dive in.","metadata":{}},{"cell_type":"code","source":"data.loc[:, ['Income','Kidhome','Teenhome','Recency',\"Clusters\"]].groupby('Clusters').describe(include='all').loc[:,(slice(None),['mean', 'min','25%', '50%', '75%'])].style.background_gradient(cmap='Greys')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[:, [\"MntWines\",'MntMeatProducts',\"NumDealsPurchases\",\"NumWebPurchases\",\"NumCatalogPurchases\",\"NumStorePurchases\",\"Clusters\"]].groupby('Clusters').describe(include='all').loc[:,(slice(None),['mean', '25%', '50%', '75%'])].style.background_gradient(cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[:, [\"NumWebPurchases\", \"NumWebVisitsMonth\",'Complain', 'Seniority', 'Age','Campaigns_Accepted', \"Clusters\"]].groupby('Clusters').describe(include='all').loc[:,(slice(None),['mean', '25%', '50%', '75%'])].style.background_gradient(cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A few quick findings - statistics:\n- Cluster 1 and 2 mostly (>75%) don't have young kids.\n- Income interquartile range for cluster 0,1,2 is around 25-40k, 50-65k, 70-85k. \n- Cluster 1 mostly (>75%) has teen. >50% cluster 0 don't have teen. Cluster 2 mostly (>75%) don't have teen.\n- Wine interquartile range for cluster 0,1,2 is around 5-50, 25-600, 350-850. \n- Deal purchase interquartile range for cluster 0,1,2 is around 1-3, 2-5, 1-1. Cluster 1 make the most deal purchases and most cluster 2 (>75%) only have 1 deal purchase.\n- Web purchase interquartile range for cluster 0,1,2 is around 1-3, 4-8, 3-6. Cluster 1 make the most web purchases and cluster 0 the least.\n- Catalog purchase interquartile range for cluster 0,1,2 is around 0-1, 2-4, 4-8. Cluster 2 make the most catalog purchases and cluster 0 the least.\n- Store purchase interquartile range for cluster 0,1,2 is around 3-4, 5-10, 6-10. Cluster 0 and 2 store purchases are very similar and cluster 0 the least.\n- Web Visit last month interquartile range for cluster 0,1,2 is around 5-8, 4-7, 1-3. Cluster 0 and 1 web visit are very similar and cluster 2 the least.\n- Seniority interquartile range for cluster 0,1,2 is around 20-70, 37-82, 22-73. Cluster 1 has slightly more longtime customer\n- Age interquartile range for cluster 0,1,2 is around 40-55, 50-65, 40-63. Cluster 1 are slightly older\n","metadata":{}},{"cell_type":"code","source":"# Countplots for discrete features\ngrh_per_row = 2\nfig, ax = plt.subplots(len(col_dis) // grh_per_row, grh_per_row, figsize=(35, 70))\n\nfor count, feature in enumerate(col_dis, 0):\n    data_copy = data.copy()\n    row = count // grh_per_row\n    col = (count) % grh_per_row\n    sns.countplot(x='Clusters', data=data, hue=feature, ax=ax[row, col])  # Specify the axis for the countplot\n    ax[row, col].set_xlabel(feature, fontsize=30)\n    ax[row, col].set_ylabel(\"Count\", fontsize=30) \n    ax[row, col].tick_params(axis='both', rotation=30, labelsize=30)  # Rotate x-axis labels and adjust label size\n    ax[row, col].legend(fontsize=30)  # Set legend label size\n\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A few quick findings - counterplots:\n- We have lots of cluster 0 people (around 50%)\n- Lots of 1-time deal purchase come from cluster 0 and 2 (probably for different reasons). \n- Cluster 1 has more 5+ deal purchases.\n- Cluster 2 has more 6+ catalog purchases.\n- Most Cluster 1 has 0 campain accepted. A bit more than 50% cluster 2 has 0 campain accepted. But, >50% of people in cluster 2 has more than 1+ campaign accepted. Most people accepted 2+ campaigns are in cluster 2.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; background-color: #FFCC66; font-size:80%; padding: 2px;border-radius:10px;\">\n    <h2> What did I seeüßê</h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: #FFF2CC; padding: 10px;\">\n<span style=\"font-size: larger;\">\n    \n### <font color=\"green\">Essence of what I've found: </font>üëì‚úç\n\n\n- We have **lots of cluster 0 people (around 45%)** (hence we don't want to give them up. There might be some potentials we could find.)\n    \n- **Cluster 0** has the **lowest income** (mean of 33,992), **Cluster 2** has the **highest**(mean of 77589).\n    \n- **Cluster 0** has the **lowest meat and wine purchases** (median 50% have 8-51 wine purchasesÔºâ, **Cluster 2** has the **highest** (median 50% have 377-864 wine purchases).\n    \n- **Cluster 0 mostly (a little less than 80%) has young kids at home**. **Cluster 1 and 2 mostly (>75%) don't** have young kids.\n    \n- **Cluster 0** people are **slightly younger**(median 50% 40 to 55 and more mid 40 to 50 than other clusters) and **cluster 1** are **slightly older**(median 50% 50-65 and more 60-70 than other clusters).\n  \n- **Cluter 0 and 1 web visit are very similar** and **cluster 2 the least**.\n\n- **Cluster 1 mostly(>75%) has teen at home**. **Cluster 2 mostly(>75%) don't have teens**.\n    \n- **Cluster 1** has the **highest web purchases** in the past two years (median 50% has 4-8) and has the **most 5+ deal purchases**. \n    \n- **Cluster 1** tends to have more ** longtime customers (80-100 weeks)** than other groups.\n    \n- **Most 1-time deal purchases** are in **cluster 0 and 2** group. (maybe for different reasons ;( )  For cluster 2, lots of deal purchases are just 1-time.\n    \n- Lots of **high (6+) catalog purcahses** happen in **cluster 2**.\n \n- **Cluster 2 people involved in campaign the most**. >50% of them has at least one campaign accepted and most 2+ campaign accepted happens in this group.\n\n    \n    \n\n### <font color=\"green\">Customer's Cluster Profiling</font>üôÜ‚Äç‚ôÇÔ∏èüôã‚Äç‚ôÄÔ∏èüôãüôã‚Äç‚ôÇÔ∏èüë®‚Äçüë©‚Äçüëßüë®‚Äçüë©‚Äçüëß‚Äçüë¶\n    \n- **Cluster 0**: Lots of (45%) customers come from this group and they have slightly more new customers. They seem to be slightly younger folks with relatively lower income. Most of them have young kids at home. Like cluster 1, they do like to surf on the company's website for deals but may not have strong purchase power. So, they make the least purchases.\n\n- **Cluster 1**: 31% of customers come form this group and they have a bit more long-time customers. They seem to be slightly older people with median income so they make the median purchases. Most of them have teen(s) at home. They like to surf on the web and buy more deal products.\n    \n- **Cluster 2**: 24% of customers come form this group. They seem to have the highest income and make the most purchases. Most of them don't have kids or teens. 6+ catalog purchases and 2+ campaign accepted mostly happen in this group. They aren't big fan of the store's web visits so their web purchases is slightly lower than the cluster 1 people. They are either busy or prefer traditional sale channels.\n    \n    \n\n### <font color=\"green\">Possible store's actions?</font>üèãüèæ‚Äç‚ôÄÔ∏èüèãüèæ\n    \nThere might be some actions the store could take to meet the diverse needs of each cluster and optimize the store's marketing and customer engagement efforts. Below I listed a few possible actions: ü•Ç\n\n- **Higher-Income Individuals (Cluster 2)**:\n  - They don't visit the store's website frequently, the store could focus on exclusive catalog offerings and campaigns delivered through traditional channels for them.\n  - They have higher income and make lots of purchases. The store could emphasize a bit on high-quality (or higher-end) products for them.\n \n- **Lower and Median Income Individuals (Clusters 0 and 1)**:\n  - Most of them has kids or teens and might be tight on money. The store could emphasize affordability and family-friendly options for them.\n    \n  - They enjoy store website and engage more in deals. The store could promote more online deals and enhance the online shopping experience specifically towards them.\n\n- **Cluster 1 older customers**\n  - Cluster 1 have more longtime and older customers, the store could promote loyalty programs, and senior discounts to enhance customer retention and satisfaction for them.\n\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"19\"></a>\n<div style=\"text-align: center; background-color: #CABD80; font-size:100%; padding: 5px;border-radius:10px 10px;\">\n    <h1> Part 5: Conclusion - A few ending thoughts</h1>\n</div> <a id=\"1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"\nWe've seen how PCA and KMeans clustering can be powerful in customer segmentation analysis that help business.\n\nIt's good to remember that the application of machine learning models should be closely aligned with business interests. For instance, I combined different campaigns for this project. However, in a business context focused on enhancing specific campaigns for customers, this approach might not be suitable.\n\nIn the real world, achieving optimal results requires extensive collaboration across departments.The key is to tailor the analysis to address the unique goals and priorities of the business. This emphasizes the importance of ongoing collaboration between data scientists and business stakeholders to ensure that machine learning models generate insights that directly contribute to the organization's strategic objectives.\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"20\"></a>\n# References and appreciations:üë£\n- Articles:\n  - https://thecleverprogrammer.com/2021/02/08/customer-personality-analysis-with-python/\n\n- Fellow Kagglers' work: \n  - https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering <br>\n  - https://www.kaggle.com/code/heyrobin/customer-segmentation-christmas-theme/comments#2616981","metadata":{}}]}